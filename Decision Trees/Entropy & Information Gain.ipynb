{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We learned about the [Gini Impurtiy Index](https://sites.google.com/view/stevenloaiza/machine-learning/decision-trees/gini-impurity-index?authuser=0) as one of the metrics to optimally choose splits in decision trees. Information Gain is yet another method that can also be used to optimally choose which feature to split the dataset on. Before we go on to learn about Information gain, we must first discuss Entropy, which was introduced by Shannon(1948).\n",
    "\n",
    "## Entropy\n",
    "\n",
    "[Definition](https://en.wikipedia.org/wiki/Entropy_(information_theory)): [E]ntropy provides an absolute limit on the shortest possible average length of a lossless compression encoding of the data produced by a source, and if the entropy of the source is less than the channel capacity of the comunication channel,the data generated by the source can be reliably communicated to the receiver.\n",
    "\n",
    "The above definition is extrememly difficult to understand, and the exact definition is not neccessarily pertinant to our discussions of decision trees. Shannon(1948) used the concept of entropy for the theory of communication, to determine how to send encoded (bits) informaiton from a sender to a receiver without loss of information and with the minimum amount of bits.\n",
    "\n",
    "Please take a look at [Demystifying Entropy](https://towardsdatascience.com/demystifying-entropy-f2c3221e2550)  and [The intuition behind Shannon's Entropy](https://towardsdatascience.com/the-intuition-behind-shannons-entropy-e74820fe9800) for an easily to understand explanation.\n",
    "\n",
    "#### Bits\n",
    "\n",
    "What are bits? We usually have TRUE or FALSE when using if statements which takes on 1 bit of data. A Bit takes on a single binary value 0(FALSE) or 1(TRUE). See the table below to understand how the storage capabilties increase with each bit.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{vmatrix}\n",
    "\\mathbf{bits} & \\mathbf{Values}  \\\\\n",
    "1 & 0 ,1  \\\\\n",
    "2 & 00 , 01, 10, 11  \\\\\n",
    "3 & 000, 001, 010, 011, 100, 101, 110, 111  \\\\\n",
    "4 & ...\\\\\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}\n",
    "\\\\\n",
    "\n",
    "$$ 2^x = n \\\\\n",
    "ln(2^x)=ln(n)\\\\\n",
    "x \\cdot ln(2) = ln (n) \\\\\n",
    "x = \\frac{ln(n)} {ln(2}=log_2(n)$$\n",
    "#### Lossless\n",
    "This concept simply means that no information was loss in the transmission from sender to receiver.\n",
    "\n",
    "#### Formula\n",
    "\n",
    "$$Entropy = - \\sum_i P(i) \\cdot log_2P(i)$$\n",
    "The formula above gives us the minimum average encoding size , which uses the minimum encoding size for each message type.\n",
    "\n",
    "High Entropy : More uncertanty\n",
    "\n",
    "Low Entropy : More Predictability\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Emotion Temperature StayHome\n",
      "1     sick        over        Y\n",
      "4  notsick        over        Y\n",
      "5     sick        over        N\n",
      "7  notsick        over        Y\n",
      "0     sick       under        N\n",
      "2  notsick       under        Y\n",
      "3  notsick       under        N\n",
      "6  notsick       under        N\n"
     ]
    }
   ],
   "source": [
    "# Create Sample \n",
    "import pandas as pd\n",
    "data={'Emotion':['sick','sick','notsick','notsick','notsick','sick','notsick','notsick'],'Temperature':['under','over','under','under','over','over','under','over'],'StayHome':['N','Y','Y','N','Y','N','N','Y']}\n",
    "df=pd.DataFrame (data)\n",
    "#sort it by Emotion\n",
    "df.sort_values(['Emotion'],inplace=True)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be calculating the entropy for each split. For instance we are going to have two split for the column \"Emotion\". One on \"notsick\" and the other on \"sick\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Emotion Temperature StayHome\n",
      "5    sick        over        N\n",
      "0    sick       under        N\n",
      "1    sick        over        Y \n",
      "\n",
      "   Emotion Temperature StayHome\n",
      "3  notsick       under        N\n",
      "6  notsick       under        N\n",
      "4  notsick        over        Y\n",
      "7  notsick        over        Y\n",
      "2  notsick       under        Y \n",
      "\n",
      "   Emotion Temperature StayHome\n",
      "3  notsick       under        N\n",
      "6  notsick       under        N\n",
      "4  notsick        over        Y\n",
      "7  notsick        over        Y\n",
      "2  notsick       under        Y\n"
     ]
    }
   ],
   "source": [
    "print(df[df.Emotion=='sick'],\"\\n\")\n",
    "print(df[df.Emotion=='notsick'],\"\\n\")\n",
    "print(df[df.Emotion=='notsick'].sort_values('StayHome'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once each split we can calculate the entropy on the target varible \"stayhome\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.array([[1,0],[0,1]])\n",
    "Y=np.array([[2,1],[1,2]]) \n",
    "Z=np.dot(X,Y)\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def entropy(col,df):\n",
    "# Takes the column number and data frame as an input\n",
    "    #Grabs the column we specify and the last column (which we assume is the decision col)\n",
    "    new_df=df.iloc[:,[col,len(df.columns)-1]]\n",
    "    #rename the columns\n",
    "    new_df.columns=('col1','col2')\n",
    "    #return the unique values in this feature along with their counts\n",
    "    names,count=np.unique(new_df.col1,return_counts=True)\n",
    "    #create an empty list\n",
    "    entropy_list=list()\n",
    "    \n",
    "    #for loop on each split to get the entry at the split\n",
    "    for i in range(0,(len(names))):\n",
    "            dff=new_df[new_df.col1==names[i]]\n",
    "            den=len(dff)\n",
    "            columns=new_df.col2.unique()\n",
    "            p1 = dff.col2.eq(columns[0]).sum()/den\n",
    "            p2 = dff.col2.eq(columns[1]).sum()/den\n",
    "            P=[p1,p2]\n",
    "            ent=0\n",
    "            entropy_list.append(count[i])\n",
    "            entropy_list.append(names[i])\n",
    "            k=0\n",
    "            for p in P:\n",
    "                k=k+1\n",
    "                ent += -p * math.log(p,2)\n",
    "                if(k==len(P)):\n",
    "                    entropy_list.append(ent)\n",
    "    return entropy_list\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy for Emotion Split ['notsick', 1.0]\n",
      "Confirm 0.9709505944546686 \n",
      "\n",
      "Entropy for Emotion Split ['sick', 1.0]\n",
      "Confirm 0.9182958340544896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Entropy for Emotion Split\",entropy(0,df)[1:3])\n",
    "print(\"Confirm\",-(3/5)*math.log((3/5),2)-(2/5)*math.log((2/5),2),\"\\n\")\n",
    "print(\"Entropy for Emotion Split\",entropy(0,df)[4:6])\n",
    "print(\"Confirm\",-(1/3)*math.log((1/3),2)-(2/3)*math.log((2/3),2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Gain\n",
    "\n",
    "Now that we have discussed Entropy we can move forward into information gain. This is the concept of a decrease in entropy after splitting the data on a feature. The greater the information gain, the greater the decrease in entropy or uncertanty. \n",
    "\n",
    "$$InformationGain(T,X) = Entropy(T) - \\sum_{splits}\\frac{s_1}{T}Entropy(s_1)$$\n",
    "\n",
    "-  T: Target population prior to the split $T= \\sum_{All Splits}$. \n",
    "-  Entropy(T): Measure the disorder before the split\n",
    "-  $s_i$: is the number of observations on the $i^{th}$ split\n",
    "-  Entropy($s_i$): Meausres the disorder for the target variable on split $s_1$\n",
    "\n",
    "Given the Example above T=8, $s_1=5$, $s_2=3$, Entropy(s_1) = 0.9709...$ $Entropy(s_2) = 0.91829...$.\n",
    "Its is difficult to tell but even when we split the original dataset using the feature \"Emotion\", we are not gaining much information to have a homogenous bucket (pure set to identify either 'N' or 'Y')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Emotion Temperature StayHome\n",
      "0  notsick       under        N\n",
      "1  notsick       under        N\n",
      "2     sick        over        N\n",
      "3     sick       under        N\n",
      "4  notsick        over        Y\n",
      "5  notsick        over        Y\n",
      "6  notsick       under        Y\n",
      "7     sick        over        Y \n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "  Emotion Temperature StayHome\n",
      "0    sick        over        N\n",
      "1    sick       under        N\n",
      "2    sick        over        Y \n",
      "\n",
      "   Emotion Temperature StayHome\n",
      "0  notsick       under        N\n",
      "1  notsick       under        N\n",
      "2  notsick        over        Y\n",
      "3  notsick        over        Y\n",
      "4  notsick       under        Y\n"
     ]
    }
   ],
   "source": [
    "print(df.sort_values('StayHome').reset_index(drop=True),\"\\n\")\n",
    "print('-----------------------------------------------------------------------------')\n",
    "print(df[df.Emotion=='sick'].sort_values('StayHome').reset_index(drop=True),\"\\n\")\n",
    "print(df[df.Emotion=='notsick'].sort_values('StayHome').reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(col,df,option=2):\n",
    "# Takes the column number and data frame as an input\n",
    "    #Grabs the column we specify and the last column (which we assume is the decision col)\n",
    "    new_df=df.iloc[:,[col,len(df.columns)-1]]\n",
    "    #rename the columns\n",
    "    new_df.columns=('col1','col2')\n",
    "    #return the unique values in this feature along with their counts\n",
    "    names,count=np.unique(new_df.col1,return_counts=True)\n",
    "    \n",
    "    \n",
    "    #create an empty list\n",
    "    entropy_list=list()\n",
    "    \n",
    "    #for loop on each split to get the entry at the split\n",
    "    for i in range(0,(len(names))):\n",
    "            if(option==2):\n",
    "                dff=new_df[new_df.col1==names[i]]\n",
    "                entropy_list.append(count[i])\n",
    "            else:\n",
    "                dff=new_df\n",
    "            den=len(dff)\n",
    "            columns=new_df.col2.unique()\n",
    "            p1 = dff.col2.eq(columns[0]).sum()/den\n",
    "            p2 = dff.col2.eq(columns[1]).sum()/den\n",
    "            P=[p1,p2]\n",
    "            ent=0\n",
    "            k=0\n",
    "            for p in P:\n",
    "                k=k+1\n",
    "                ent += -p * math.log(p,2)\n",
    "                if(k==len(P)):\n",
    "                    entropy_list.append(ent)\n",
    "    return entropy_list\n",
    "    \n",
    "def information_gain(col,df):\n",
    "    den=len(df)\n",
    "\n",
    "    info_gain = entropy(col,df,1)[1]\n",
    "    Split=entropy(col,df,2)\n",
    "    c=len(Split)/2\n",
    "    c=int(c)\n",
    "    \n",
    "    j=0\n",
    "    \n",
    "    for i in range(0,c):\n",
    "        weight=Split[j]/den\n",
    "        info_gain= info_gain - weight*Split[j+1]\n",
    "        j=j+2\n",
    "    return(info_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Algorithm states the info gain is:  0.048794940695398525\n",
      "Check:  0.04879494069539847\n"
     ]
    }
   ],
   "source": [
    "print(\"The Algorithm states the info gain is: \",information_gain(0,df))\n",
    "#confrim\n",
    "EntropyT =-(4/8)*math.log((4/8),2)-(4/8)*math.log((4/8),2)\n",
    "EntropyS1=-(3/5)*math.log((3/5),2)-(2/5)*math.log((2/5),2)\n",
    "EntropyS2=-(1/3)*math.log((1/3),2)-(2/3)*math.log((2/3),2)\n",
    "print(\"Check: \", 1-((5/8)*EntropyS1+(3/8)*EntropyS2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the infromation we gain is minimal on the split \"Emotion\". Could we have down better by splitting on Tempreature instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain from Splitting on Temp:  0.1887218755408671\n"
     ]
    }
   ],
   "source": [
    "print(\"Information Gain from Splitting on Temp: \",information_gain(1,df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a great improvement on the amount of information we gained. Lets take a look at this in the table format. As you can tell we have gone from an even split in the orginal dataset, to a 25% / 75% split once we conditioned on Temperature. Therefore, we have gain more information because we are able to place each predictor into a bucket with similar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Emotion Temperature StayHome\n",
      "0  notsick       under        N\n",
      "1  notsick       under        N\n",
      "2     sick        over        N\n",
      "3     sick       under        N\n",
      "4  notsick        over        Y\n",
      "5  notsick        over        Y\n",
      "6  notsick       under        Y\n",
      "7     sick        over        Y \n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "   Emotion Temperature StayHome\n",
      "0     sick        over        N\n",
      "1  notsick        over        Y\n",
      "2  notsick        over        Y\n",
      "3     sick        over        Y \n",
      "\n",
      "   Emotion Temperature StayHome\n",
      "0  notsick       under        N\n",
      "1  notsick       under        N\n",
      "2     sick       under        N\n",
      "3  notsick       under        Y\n"
     ]
    }
   ],
   "source": [
    "print(df.sort_values('StayHome').reset_index(drop=True),\"\\n\")\n",
    "print('-----------------------------------------------------------------------------')\n",
    "print(df[df.Temperature=='over'].sort_values('StayHome').reset_index(drop=True),\"\\n\")\n",
    "print(df[df.Temperature=='under'].sort_values('StayHome').reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are times when a bucket is able to completely isolate one of the decision parameters and correctly identify it. When this occurs the probabily will of the other parameters occuring is 0. We arent able to take the $log (0)$ because it creates a $-inf$. \n",
    "\n",
    "Additionally, the code above is hard coded to work on features with only two possible outcomes. When we move into Decison Trees we will generalize the entropy function to take into account n possible outcomes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
